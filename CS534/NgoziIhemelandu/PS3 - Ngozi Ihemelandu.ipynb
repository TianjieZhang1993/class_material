{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Ngozi Ihemelandu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VuWANcq7m1KQ"
   },
   "source": [
    "# CS534 Homework 3\n",
    "\n",
    "<font color='red'>Good job</font> \n",
    "Put your homework in the directory with your name. Please mentionin this file the names of any students with whom you collaborated. If you didn't collaborate with anyone, mark your collaborators as \"None.\" Remember, your goal is to communicate. Full credit will be given only to correct solutions which are described clearly. Convoluted and obtuse descriptions will receive low marks. To complete your homework, you may ONLY consult the following material:\n",
    "\n",
    "lecture slides course notes you or others took during lecture. the required text (CLRS) websites that may clarify the concepts covered in the material but do not in any way provide complete solutions to the problems. Deadline 2/20/2019\n",
    "\n",
    "Please provide an answer to the following question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9_7-Wkkm1KR"
   },
   "source": [
    "# Question 1 (15 pts)\n",
    "<font color='red'> the procedure to find the best transformation as a part of the training procedure is not provided -2 </font>\n",
    "\n",
    "To classify iris dataset (Iris-Versicolor vs. others) in the best way, you have to create an algorithm able to determine (with the k-fold cross validation) what is the best space transformation (among rbf_kernel, polynomial features, and polynomial kernel) and its hyperparameters. Each transformation has its own parameters rbf_kernel->gamma, polynomial-features-> degree, polynomial_kernel ->(gamma, degree). The performance has to be tested of the entire algorithm with another K-fold cross-validation (please use then trainset, validationset, and testset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import rbf_kernel, polynomial_kernel\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_fscore_support,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal-length  sepal-width  petal-length  petal-width        class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the iris data\n",
    "filename = \"iris.csv\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "data_iris = pd.read_csv(filename, names=names)\n",
    "data_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the independent and dependent variables into numpy arrays\n",
    "indp_var = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width']\n",
    "X = data_iris[indp_var].values\n",
    "Y = data_iris[\"class\"]=='Iris-versicolor'\n",
    "\n",
    "#classify iris dataset (Iris-Versicolor vs. others)\n",
    "Y = Y.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible values of parameters to optimize over for space transformation \n",
    "# (rbf_kernel, polynomial features, and polynomial kernel)\n",
    "\n",
    "rbf_param = {'gamma': [0.0001, 0.001, 0.01, 0.1, 0.5, 0.9, 1,10, 100, 1000]}\n",
    "polyKernel_param = {'gamma': [0.001, 0.01, 0.1, 0.5, 0.9, 1, 10], 'degree': [1,2,3,4,5]}\n",
    "polyFeatures_param = {'degree': [1,2,3,4,5]}\n",
    "\n",
    "param_grid = dict(rbf=rbf_param, poly_Kernel=polyKernel_param, poly_Features=polyFeatures_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter Selection\n",
    "def model_selection(X_train, y_train, Parameters, n_splits, kFunction):\n",
    "    \n",
    "    n_splits = n_splits\n",
    "    folds_scores = {} \n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=123, shuffle=True) #Stratified Cross Validation \n",
    "    cvFolds = list(skf.split(X_train, y_train))\n",
    "    for count, fold in enumerate(cvFolds):\n",
    "        \n",
    "        train_index = fold[0]\n",
    "        test_index = fold[1]\n",
    "        \n",
    "        cvX_train, cvX_test = X_train[train_index], X_train[test_index]\n",
    "        cvy_train, cvy_test = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        parameters_scores = {} # dict to hold score for every choice of parameter value\n",
    "        foldi = \"fold\"+str(count+1)\n",
    "       \n",
    "        #Learn a model on cvX_train for every choice of hyper-parameter value pi(Parameter)\n",
    "        for Parameter in Parameters:\n",
    "                        \n",
    "            if kFunction == \"rbf\":    # rbf_kernel\n",
    "                \n",
    "                # Get the (N x N) Gram matrix(Kernel matrix) for every data point pair using the rbf kernel function\n",
    "                # where N is the number of data points in the training data set\n",
    "                cvXK_train = rbf_kernel(cvX_train, gamma=Parameter)\n",
    "                \n",
    "                # Get the (M x N) Gram matrix(Kernel matrix) for every data point pair using the Polynomial Kernel function\n",
    "                # where M is the number of data points in the validation data set and N points in train set             \n",
    "                cvXK_test = rbf_kernel(cvX_test,cvX_train, gamma=Parameter)\n",
    "                \n",
    "                # Set kernel='precomputed'and pass the Gram matrix(kernel matrix) [cvXK_train] instead of \n",
    "                # the train set [cvX_train] in the fit method                 \n",
    "                kernel_type = 'precomputed'\n",
    "                                \n",
    "            if kFunction == \"poly_Kernel\":    # Polynomial Kernel\n",
    "                \n",
    "                gamma = Parameter[0]\n",
    "                degree = Parameter[1]\n",
    "                \n",
    "                # Get the (N x N) Gram matrix(Kernel matrix) for every data point pair using the Polynomial Kernel function\n",
    "                # where N is the number of data points in the training data set\n",
    "                cvXK_train = polynomial_kernel(cvX_train, gamma=gamma, degree=degree)\n",
    "\n",
    "                # Get the (M x N) Gram matrix(Kernel matrix) for every data point pair using the Polynomial Kernel function\n",
    "                # where M is the number of data points in the validation data set and N the number of points in the train set\n",
    "                cvXK_test = polynomial_kernel(cvX_test,cvX_train, gamma=gamma, degree=degree)\n",
    " \n",
    "                # Set kernel='precomputed'and pass the Gram matrix(kernel matrix) [cvXP_train] instead of \n",
    "                # the train set [cvX_train] in the fit method \n",
    "                kernel_type = 'precomputed' \n",
    "                                              \n",
    "            if kFunction == \"poly_Features\":   # Polynomial features\n",
    "                \n",
    "                # Generate a new feature matrix \n",
    "                poly = PolynomialFeatures(Parameter)\n",
    "                cvXK_train = poly.fit_transform(cvX_train)\n",
    "                cvXK_test = poly.transform(cvX_test)\n",
    "                \n",
    "                # fit model using the transformed feature space\n",
    "                kernel_type = 'linear' \n",
    " \n",
    "            clf = svm.SVC(random_state=0, kernel=kernel_type).fit(cvXK_train,cvy_train) #learn model\n",
    "            cvy_test_pred = clf.predict(cvXK_test) #apply model on validation set\n",
    "\n",
    "            # compute the accuracy rate corresponding to model learnt using pi(Parameter)\n",
    "            parameters_scores[Parameter] = accuracy_score(cvy_test, cvy_test_pred)\n",
    "\n",
    "        folds_scores[foldi] = parameters_scores    # Accuracy for every choice of hyper-parameter value per fold \n",
    "        \n",
    "    # Compute the Validation error rate(Average accuracy rate)\n",
    "    df_params = pd.DataFrame.from_dict(folds_scores)\n",
    "    series_params = df_params.sum(axis = 1)/n_splits\n",
    "    \n",
    "    # Select the best hyper-parameter value\n",
    "    best_param = series_params.idxmax()\n",
    "     \n",
    "    # Learn final model on the train dataset\n",
    "    if kFunction == \"rbf\":\n",
    "        XK_train = rbf_kernel(X_train, gamma=best_param)\n",
    "    \n",
    "    if kFunction == \"poly_Kernel\":\n",
    "        \n",
    "        gamma = best_param[0]\n",
    "        degree = best_param[1]       \n",
    "        XK_train = polynomial_kernel(X_train, gamma=gamma, degree=degree) \n",
    "\n",
    "    if kFunction == \"poly_Features\":\n",
    "        # Generate a new feature matrix \n",
    "        poly = PolynomialFeatures(best_param)\n",
    "        XK_train = poly.fit_transform(X_train)\n",
    "        \n",
    "    clf = svm.SVC(random_state=0, kernel=kernel_type).fit(XK_train,y_train)\n",
    "    return(clf, best_param)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outer-fold1</th>\n",
       "      <th>outer-fold2</th>\n",
       "      <th>outer-fold3</th>\n",
       "      <th>outer-fold4</th>\n",
       "      <th>outer-fold5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>poly_Features</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poly_Kernel</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rbf</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               outer-fold1  outer-fold2  outer-fold3  outer-fold4  outer-fold5\n",
       "poly_Features     1.000000     0.933333     0.966667          1.0     0.933333\n",
       "poly_Kernel       1.000000     0.900000     1.000000          1.0     0.900000\n",
       "rbf               0.966667     0.900000     0.966667          1.0     0.933333"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nested Cross Validation\n",
    "import itertools\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=123, shuffle=True) #Stratified Cross Validation \n",
    "outerfolds_scores = dict()\n",
    "folds_hyperparameters = dict()\n",
    "\n",
    "innerCV_splits = 5\n",
    "\n",
    "cvFolds = list(skf.split(X, Y))\n",
    "\n",
    "for count, fold in enumerate(cvFolds):\n",
    "\n",
    "    train_index = fold[0]\n",
    "    test_index = fold[1]\n",
    "    \n",
    "    cvX_train, cvX_test = X[train_index], X[test_index]\n",
    "    cvy_train, cvy_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    foldi = \"outer-fold\"+str(count+1)\n",
    "    \n",
    "    #scale the data set (both train and test)\n",
    "    scaler = StandardScaler()\n",
    "    cvX_train = scaler.fit_transform(cvX_train) \n",
    "    cvX_test = scaler.transform(cvX_test)\n",
    "    \n",
    "    kalgo_scores = {}    # holds accuracy rate for each space transformation algorithm\n",
    "    kpi = {}  # Best hyper-parameter for each space transformation algorithm\n",
    "    \n",
    "    for key in param_grid.keys():\n",
    "\n",
    "        if key in [\"rbf\", \"poly_Kernel\"]:\n",
    "            gamma_range = param_grid[key][\"gamma\"]\n",
    "\n",
    "        if key in [\"poly_Features\", \"poly_Kernel\"]:\n",
    "            degree_range = param_grid[key][\"degree\"]\n",
    "            \n",
    "        if key == \"rbf\":    # rbf_kernel\n",
    "            \n",
    "            param_sets = gamma_range\n",
    "            \n",
    "        if key == \"poly_Kernel\":    # Polynomial Kernel\n",
    "\n",
    "            # Permutation of the gamma_range and degree_range\n",
    "            permu = [list(zip(x,degree_range)) for x in itertools.permutations(gamma_range,len(degree_range))]\n",
    "            # flatten the list of lists and then get only unique combinations\n",
    "            param_sets = set(list(itertools.chain(*permu)))\n",
    "            \n",
    "        if key == \"poly_Features\":   # Polynomial features\n",
    "            param_sets = degree_range\n",
    "            \n",
    "        # Inner cross-validation to select the best hyper-parameter value and model learned over cvX_train data set\n",
    "        model_pi = model_selection(cvX_train, cvy_train, param_sets, innerCV_splits, key)\n",
    "        \n",
    "        # Compute accuracy rate  using test set\n",
    "        model = model_pi[0]\n",
    "        pi = model_pi[1]\n",
    "        \n",
    "        if key == \"rbf\": \n",
    "            # Get the (M x N) Gram matrix(Kernel matrix) data points in the validation/train data set \n",
    "            # for best hyper-parameter pi\n",
    "            cvXK_test = rbf_kernel(cvX_test,cvX_train, gamma=pi)          \n",
    "        if key == \"poly_Kernel\":\n",
    "            gamma = pi[0]\n",
    "            degree = pi[1] \n",
    "            # Get the (M x N) Gram matrix(Kernel matrix) data points in the validation/train data set \n",
    "            # for best hyper-parameter pi\n",
    "            cvXK_test = polynomial_kernel(cvX_test,cvX_train, gamma=gamma, degree=degree)\n",
    "        if key == \"poly_Features\":\n",
    "            # Get new feature space for best hyper-parameter pi\n",
    "            poly = PolynomialFeatures(pi)\n",
    "            cvXK_test = poly.fit_transform(cvX_test)\n",
    "            \n",
    "        cvy_test_pred = model.predict(cvXK_test)\n",
    "        kalgo_scores[key] = accuracy_score(cvy_test, cvy_test_pred)\n",
    "        kpi[key] = pi   # holds best hyperparameters for each space transformation (rbf,poly_Features,poly_Kernel)\n",
    "    \n",
    "    outerfolds_scores[foldi] = kalgo_scores    # Accuracy rate per fold for each key(algorithm)\n",
    "    folds_hyperparameters[foldi] = kpi\n",
    "    \n",
    "#Accuracy rate across each outer fold for [rbf/poly_Kernel/poly_Features]\n",
    "df_outerFolds = pd.DataFrame.from_dict(outerfolds_scores)\n",
    "df_outerFolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best hyper-parameters for each space transformation (rbf,poly_Features,poly_Kernel) across the outer folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outer-fold1</th>\n",
       "      <th>outer-fold2</th>\n",
       "      <th>outer-fold3</th>\n",
       "      <th>outer-fold4</th>\n",
       "      <th>outer-fold5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>poly_Features</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poly_Kernel</th>\n",
       "      <td>(0.5, 2)</td>\n",
       "      <td>(0.1, 5)</td>\n",
       "      <td>(0.5, 2)</td>\n",
       "      <td>(1.0, 2)</td>\n",
       "      <td>(0.5, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rbf</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              outer-fold1 outer-fold2 outer-fold3 outer-fold4 outer-fold5\n",
       "poly_Features           2           2           2           2           2\n",
       "poly_Kernel      (0.5, 2)    (0.1, 5)    (0.5, 2)    (1.0, 2)    (0.5, 2)\n",
       "rbf                   0.5         0.5         0.5         0.5         0.5"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyperpar = pd.DataFrame.from_dict(folds_hyperparameters)\n",
    "df_hyperpar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select the algorithm that has the maximum Average Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poly_Features    0.966667\n",
       "poly_Kernel      0.960000\n",
       "rbf              0.953333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ave_Scores = df_outerFolds.sum(axis = 1)/n_splits\n",
    "Ave_Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'poly_Features'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ave_Scores.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBCQhHD-m1KS"
   },
   "source": [
    "## Question 2 (10 pts)\n",
    "Write the explicit constraints (without using any vectorial notation, as a summation of single variables multiplied by a constant + bias term) of the Support Vector Machine to classify correctly iris dataset (Iris-Versicolor vs. others). In particular use 5 points in Iris-Versicolor, 2 points for iris-setosa, and 3 points for iris Virginia.\n",
    "Please show the points you selected and after the constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\displaystyle {\\vec {w}}\\cdot {\\vec {x}}_{i}-b\\geq 1}$, if ${\\displaystyle y_{i}=1}$(class = Iris-versicolor)<br> or <br>\n",
    "${\\displaystyle {\\vec {w}}\\cdot {\\vec {x}}_{i}-b\\leq -1}$, if ${\\displaystyle y_{i}=-1}$(class = ~Iris-versicolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal-length  sepal-width  petal-length  petal-width            class\n",
       "84           5.4          3.0           4.5          1.5  Iris-versicolor\n",
       "59           5.2          2.7           3.9          1.4  Iris-versicolor\n",
       "51           6.4          3.2           4.5          1.5  Iris-versicolor\n",
       "74           6.4          2.9           4.3          1.3  Iris-versicolor\n",
       "77           6.7          3.0           5.0          1.7  Iris-versicolor"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Iris_versicolor = data_iris[data_iris['class'] == \"Iris-versicolor\"]\n",
    "nrows = Iris_versicolor.shape[0]\n",
    "idx = np.random.randint(nrows, size=5)\n",
    "Iris_versicolor.iloc[idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5.4w_{1} + 3.0w_{2} + 4.5w_{3} + 1.5w_{4}-b\\geq 1$<br>\n",
    "$5.2w_{1} + 2.7w_{2} + 3.9w_{3} + 1.4w_{4}-b\\geq 1$<br>\n",
    "$6.4w_{1} + 3.2w_{2} + 4.5w_{3} + 1.5w_{4}-b\\geq 1$<br>\n",
    "$6.4w_{1} + 2.9w_{2} + 4.3w_{3} + 1.3w_{4}-b\\geq 1$<br>\n",
    "$6.7w_{1} + 3.0w_{2} + 5.0w_{3} + 1.7w_{4}-b\\geq 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal-length  sepal-width  petal-length  petal-width        class\n",
       "41           4.5          2.3           1.3          0.3  Iris-setosa\n",
       "19           5.1          3.8           1.5          0.3  Iris-setosa"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Iris_setosa = data_iris[data_iris['class'] == \"Iris-setosa\"]\n",
    "nrows = Iris_setosa.shape[0]\n",
    "idx = np.random.randint(nrows, size=2)\n",
    "Iris_setosa.iloc[idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4.5w_{1} + 2.3w_{2} + 1.3w_{3} + 0.3w_{4}-b \\leq -1$<br>\n",
    "$5.1w_{1} + 3.8w_{2} + 1.5w_{3} + 0.3w_{4}-b \\leq -1$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal-length  sepal-width  petal-length  petal-width           class\n",
       "105           7.6          3.0           6.6          2.1  Iris-virginica\n",
       "142           5.8          2.7           5.1          1.9  Iris-virginica\n",
       "105           7.6          3.0           6.6          2.1  Iris-virginica"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Iris_virginica = data_iris[data_iris['class'] == \"Iris-virginica\"]\n",
    "nrows = Iris_virginica.shape[0]\n",
    "idx = np.random.randint(nrows, size=3)\n",
    "Iris_virginica.iloc[idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$7.6w_{1} + 3.0w_{2} + 6.6w_{3} + 2.1w_{4}-b \\leq -1$<br>\n",
    "$5.8w_{1} + 2.7w_{2} + 5.1w_{3} + 1.9w_{4}-b \\leq -1$<br>\n",
    "$7.6w_{1} + 3.0w_{2} + 6.6w_{3} + 2.1w_{4}-b \\leq -1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mHkf54Sm1KT"
   },
   "source": [
    "# Extra Points ( 5pts)\n",
    "An unbalanced dataset (e.g. 95% vs 5%) can be problematic even in the training phase. The learned function can be trivial, e.g. always predicting one class.\n",
    "A possible solution can have a weight for each point in the way that making a mistake in the minority class will coun more w.r.t. the other. Please redefine the likelihood of the logistic regression to consider these weights for each point. Please compute the log-likelihood and its derivatives.\n",
    "In addition, add to the negative log-likelihood the norm of W (sum of the square of each component) and compute the derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-balancing (weighting) of the logistic loss (negative log likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logLoss (negative log-likelihood) = logL($\\beta$)} = -\\sum_{i=1}^{n}[y_{i}log(p_{i}) + (1-y_{i})log(1-p_{i})]$$<br>\n",
    "Our event of interest (probability of success is the minority observations which is related to the first component of the negative log-likelihood: Label = 1. <br> While the second component represents the loss related to the majority observation Label = 0<br><br>\n",
    "Probability of success $p_{i} = \\dfrac{1}{1 + e^{-x_{i}\\beta}}$<br> Probability of failure $(1-p_{i}) = \\dfrac{1}{1 + e^{x_{i}\\beta}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted likelihood:<br><br>$L_{w}(\\beta) = \\displaystyle \\prod_{i=1}^{n} (p_{i})^{w_{1}y_{i}}(1-p_{i})^{(1-w_{1})(1-y_{i})}$<br><br>where $w_{1} = \\dfrac{n_{1}}{N}$, the ratio of number of observations related to the majority label over the total number of observations<br><br>\n",
    "$logL_{w}(\\beta) =\\ell_{w} = \\sum_{i=1}^{n}[w_{1}y_{i}log(p_{i}) + (1-w_{1})(1-y_{i})log(1-p_{i})]$<br><br>For weighted logistic regression, the gradient and Hessian are obtained by differentiating the regularized weighted log-likelihood with respect to $\\beta$<br><br>\n",
    "\n",
    "$$\\text{regularized weighted log-likelihood = }logL_{w}(\\beta) = \\sum_{i=1}^{n}w_{i}ln\\dfrac{e^{y_{i}x_{i}\\beta}}{1+e^{x_{i}\\beta}}-\\dfrac{\\lambda}{2}\\left\\lVert \\beta \\right\\rVert^{2}$$<br>$$\\text{where} \\dfrac{\\lambda}{2}\\left\\lVert \\beta \\right\\rVert^{2} \\text{is the regularization (penalty) term added to obtain better generalization.}$$\n",
    "<br><br>In matrix form, the gradient is:<br>$\\nabla_{\\beta}lnL_{w} = X^{T}W(y-p)-\\lambda\\beta$<br>where $W = diag(w_{i})$ and $p$ is the probability vector $\\dfrac{1}{1 + e^{-x_{i}\\beta}}$. <br><br>The Hessian with respect to $\\beta$ is then <br>$\\nabla_{\\beta}^{2}lnL_{w} = -X^{T}DX-\\lambda I$<br>where $D =diag(v_{i}w_{i})$ and $v_{i}$ is the variance of $\\beta$ \n",
    "<br><br>The Newton–Raphson update with respect to $\\beta$ on the $(c + 1)$th iteration is<br>$\\hat{\\beta}^{(c+1)} = (X^{T}DX + \\lambda I)^{-1}X^{T}Dz^{c}$<br>where $z^{c}=\\hat{\\beta}^{c}+ D^{-1}(y-p)$ is the adjusted dependent variable or the adjusted response.<br><br>The weighted least squares (WLS) subproblem is then <br>$(X^{T}DX + \\lambda I)\\hat{\\beta}^{(c+1)}=X^{T}Dz^{c}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqC9w6tkm1KU"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PS3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
