{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VuWANcq7m1KQ"
   },
   "source": [
    "# CS534 Homework 3\n",
    "\n",
    "Put your homework in the directory with your name. Please mentionin this file the names of any students with whom you collaborated. If you didn't collaborate with anyone, mark your collaborators as \"None.\" Remember, your goal is to communicate. Full credit will be given only to correct solutions which are described clearly. Convoluted and obtuse descriptions will receive low marks. To complete your homework, you may ONLY consult the following material:\n",
    "\n",
    "lecture slides course notes you or others took during lecture. the required text (CLRS) websites that may clarify the concepts covered in the material but do not in any way provide complete solutions to the problems. Deadline 2/20/2019\n",
    "\n",
    "Please provide an answer to the following question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"iris.csv\",header=None)\n",
    "data.columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "data_val=data.values\n",
    "\n",
    "def classify(s):\n",
    "    if s=='Iris-versicolor':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "X=data_val[:,0:3]\n",
    "y=data_val[:,4]\n",
    "\n",
    "y1=np.array([classify(x) for x in y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9_7-Wkkm1KR"
   },
   "source": [
    "# Question 1 (15 pts)\n",
    "\n",
    "To classify iris dataset (Iris-Versicolor vs. others) in the best way, you have to create an algorithm able to determine (with the k-fold cross validation) what is the best space transformation (among rbf_kernel, polynomial features, and polynomial kernel) and its hyperparameters. Each transformation has its own parameters rbf_kernel->gamma, polynomial-features-> degree, polynomial_kernel ->(gamma, degree). The performance has to be tested of the entire algorithm with another K-fold cross-validation (please use then trainset, validationset, and testset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_fscore_support,accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def transform_rbf(hyper1, X_train, Y_train, skf, n_splits):\n",
    "    rbf_dict={}\n",
    "    for gamma in hyper1:\n",
    "        #print(\"gamma=\",gamma)\n",
    "        #n_splits=10\n",
    "        #skf = StratifiedKFold(n_splits=n_splits,random_state=10,shuffle=True)\n",
    "        #skf = KFold(n_splits=n_splits,random_state=10,shuffle=True)\n",
    "        c=0\n",
    "        for train_index, val_index in skf.split(X_train, Y_train):\n",
    "            X_train1=X_train[train_index]\n",
    "            X_val=X_train[val_index]\n",
    "            y_train=Y_train[train_index]\n",
    "            y_val=Y_train[val_index]\n",
    "            scaler = StandardScaler()\n",
    "            XS_train=scaler.fit_transform(X_train1) \n",
    "            XS_val=scaler.transform(X_val)\n",
    "            XSK_train=rbf_kernel(XS_train, gamma=gamma)\n",
    "            XSK_test=rbf_kernel(XS_val,XS_train, gamma=gamma)\n",
    "            \n",
    "            clf = LogisticRegression(random_state=0, solver='lbfgs').fit(XSK_train,y_train)\n",
    "            y_pred=clf.predict(XSK_test) \n",
    "            #print(gamma,accuracy_score(y_val, y_pred))\n",
    "            c+=accuracy_score(y_val, y_pred)\n",
    "            #c+=f1_score(y_val, y_pred, average=\"macro\")\n",
    "            \n",
    "        accuracyavg=c/n_splits\n",
    "        rbf_dict[gamma]=accuracyavg\n",
    "   \n",
    "    # Cycle through dictionary to highlight hyperparameter with highest metric (acc, F1 score, whatever)    \n",
    "    max=0\n",
    "    for key,value in rbf_dict.items():\n",
    "        #print(key,value)\n",
    "        if value > max:\n",
    "            max=value\n",
    "            max_key=key\n",
    "    #best_tup=(max_key,max)\n",
    "    \n",
    "    return max_key\n",
    "\n",
    "\n",
    "def val_rbf(X_train, Y_train,skf,gamma,n_splits):\n",
    "    \n",
    "    c=0\n",
    "    for train_index, val_index in skf.split(X_train, Y_train):\n",
    "        X_train1=X_train[train_index]\n",
    "        X_val=X_train[val_index]\n",
    "        y_train=Y_train[train_index]\n",
    "        y_val=Y_train[val_index]\n",
    "        scaler = StandardScaler()\n",
    "        XS_train=scaler.fit_transform(X_train1) \n",
    "        XS_val=scaler.transform(X_val)\n",
    "        XSK_train=rbf_kernel(XS_train, gamma=gamma)\n",
    "        XSK_test=rbf_kernel(XS_val,XS_train, gamma=gamma)\n",
    "\n",
    "        clf = LogisticRegression(random_state=0, solver='lbfgs').fit(XSK_train,y_train)\n",
    "        y_pred=clf.predict(XSK_test) \n",
    "        #print(gamma,accuracy_score(y_val, y_pred))\n",
    "        c+=accuracy_score(y_val, y_pred)\n",
    "        #c+=f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    accuracyavg=c/n_splits\n",
    "    return accuracyavg\n",
    "\n",
    "def classifier_rbf(gamma_val,X_train,X_test,y_train,y_test):\n",
    "    scaler = StandardScaler()\n",
    "    XS_train=scaler.fit_transform(X_train) \n",
    "    XS_test=scaler.transform(X_test)\n",
    "    XSK_train=rbf_kernel(XS_train, gamma=gamma_val)\n",
    "    XSK_test=rbf_kernel(XS_test,XS_train, gamma=gamma_val)\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs').fit(XSK_train,y_train)\n",
    "    y_pred=clf.predict(XSK_test) \n",
    "    hyper_param=gamma_val\n",
    "    string=\"rbf_kernel\"\n",
    "    return accuracy_score(y_test, y_pred),string\n",
    "    #return f1_score(y_test, y_pred,average=\"macro\"),string\n",
    "    \n",
    "            \n",
    "def transform_poly(hyper1, hyper2, X_train, Y_train, skf, n_splits):\n",
    "    # hyper1=degree, hyper2=gamma\n",
    "    poly_dict={}\n",
    "    param_list = [(degree,gamma) for degree in hyper1 for gamma in hyper2]\n",
    "    param_tup=tuple(param_list)\n",
    "    for tup in param_tup:\n",
    "        #n_splits=10\n",
    "        #skf = StratifiedKFold(n_splits=n_splits,random_state=10,shuffle=True)\n",
    "        #skf = KFold(n_splits=n_splits,random_state=10,shuffle=True)\n",
    "        c=0\n",
    "        for train_index, val_index in skf.split(X_train, Y_train):\n",
    "            X_train1=X_train[train_index]\n",
    "            X_val=X_train[val_index]\n",
    "            y_train=Y_train[train_index]\n",
    "            y_val=Y_train[val_index]\n",
    "            scaler = StandardScaler()\n",
    "            XS_train=scaler.fit_transform(X_train1) \n",
    "            XS_val=scaler.transform(X_val)\n",
    "            degree,gamma=tup\n",
    "            XSK_train=polynomial_kernel(XS_train,degree=degree, gamma=gamma)\n",
    "            XSK_test=polynomial_kernel(XS_val,XS_train,degree=degree, gamma=gamma)\n",
    "            \n",
    "            clf = LogisticRegression(random_state=0, solver='lbfgs').fit(XSK_train,y_train)\n",
    "            y_pred=clf.predict(XSK_test) \n",
    "            #print(gamma,accuracy_score(y_val, y_pred))\n",
    "            c+=accuracy_score(y_val, y_pred)\n",
    "            #c+=f1_score(y_val, y_pred, average=\"macro\")\n",
    "            \n",
    "        accuracyavg=c/n_splits\n",
    "        poly_dict[tup]=accuracyavg\n",
    "        \n",
    "    # Cycle through dictionary to highlight hyperparameter with highest metric (acc, F1 score, whatever)    \n",
    "    max=0\n",
    "    for key,value in poly_dict.items():\n",
    "        #print(key,value)\n",
    "        if value > max:\n",
    "            max=value\n",
    "            max_key=key\n",
    "    #best_tup=(max_key,max)\n",
    "    \n",
    "    return max_key\n",
    "\n",
    "def val_poly( X_train, Y_train,skf, param_tup, n_splits):\n",
    "    # hyper1=degree, hyper2=gamma\n",
    "    degree,gamma=param_tup\n",
    "    c=0\n",
    "    for train_index, val_index in skf.split(X_train, Y_train):\n",
    "        X_train1=X_train[train_index]\n",
    "        X_val=X_train[val_index]\n",
    "        y_train=Y_train[train_index]\n",
    "        y_val=Y_train[val_index]\n",
    "        scaler = StandardScaler()\n",
    "        XS_train=scaler.fit_transform(X_train1) \n",
    "        XS_val=scaler.transform(X_val)\n",
    "        degree,gamma=tup\n",
    "        XSK_train=polynomial_kernel(XS_train,degree=degree, gamma=gamma)\n",
    "        XSK_test=polynomial_kernel(XS_val,XS_train,degree=degree, gamma=gamma)\n",
    "\n",
    "        clf = LogisticRegression(random_state=0, solver='lbfgs').fit(XSK_train,y_train)\n",
    "        y_pred=clf.predict(XSK_test) \n",
    "        #print(gamma,accuracy_score(y_val, y_pred))\n",
    "        c+=accuracy_score(y_val, y_pred)\n",
    "        #c+=f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    accuracyavg=c/n_splits\n",
    "    return accuracyavg\n",
    "\n",
    "def classifier_polynomial_kernel(param_tup,X_train,X_test,y_train,y_test):\n",
    "    degree_val,gamma_val=param_tup\n",
    "    scaler = StandardScaler()\n",
    "    XS_train=scaler.fit_transform(X_train) \n",
    "    XS_test=scaler.transform(X_test)\n",
    "    XSK_train=polynomial_kernel(XS_train, degree=degree_val,gamma=gamma_val)\n",
    "    XSK_test=polynomial_kernel(XS_test,XS_train,degree=degree_val,gamma=gamma_val)\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs').fit(XSK_train,y_train)\n",
    "    y_pred=clf.predict(XSK_test) \n",
    "    string=\"Poly_kernel\"\n",
    "    return accuracy_score(y_test, y_pred),string\n",
    "    #return f1_score(y_test, y_pred,average=\"macro\"),string\n",
    "\n",
    "def transform_poly_features(hyper1, X_train, Y_train,skf,n_splits):\n",
    "    poly_feat_dict={}\n",
    "    for degree in hyper1:\n",
    "        #print(\"gamma=\",gamma)\n",
    "        #n_splits=10\n",
    "        #skf = StratifiedKFold(n_splits=n_splits,random_state=10,shuffle=True)\n",
    "        #skf = KFold(n_splits=n_splits,random_state=10,shuffle=True)\n",
    "        c=0\n",
    "        for train_index, val_index in skf.split(X_train, Y_train):\n",
    "            X_train1=X_train[train_index]\n",
    "            X_val=X_train[val_index]\n",
    "            y_train=Y_train[train_index]\n",
    "            y_val=Y_train[val_index]\n",
    "            scaler = StandardScaler()\n",
    "            XS_train=scaler.fit_transform(X_train1) \n",
    "            XS_val=scaler.transform(X_val)\n",
    "            poly = PolynomialFeatures(degree)\n",
    "            XSK_train=poly.fit_transform(XS_train)\n",
    "            XSK_test=poly.transform(XS_val)\n",
    "            \n",
    "            clf = LogisticRegression(random_state=0, solver='lbfgs').fit(XSK_train,y_train)\n",
    "            y_pred=clf.predict(XSK_test) \n",
    "            #print(gamma,accuracy_score(y_val, y_pred))\n",
    "            c+=accuracy_score(y_val, y_pred)\n",
    "            #c+=f1_score(y_val, y_pred, average=\"macro\")\n",
    "            \n",
    "        accuracyavg=c/n_splits\n",
    "        poly_feat_dict[degree]=accuracyavg\n",
    "   \n",
    "    # Cycle through dictionary to highlight hyperparameter with highest metric (acc, F1 score, whatever)    \n",
    "    max=0\n",
    "    for key,value in poly_feat_dict.items():\n",
    "        #print(key,value)\n",
    "        if value > max:\n",
    "            max=value\n",
    "            max_key=key\n",
    "    #best_tup=(max_key,max)\n",
    "    \n",
    "    return max_key\n",
    "\n",
    "def val_poly_features(X_train, Y_train,skf, degree, n_splits):\n",
    "    \n",
    "    c=0\n",
    "    for train_index, val_index in skf.split(X_train, Y_train):\n",
    "        X_train1=X_train[train_index]\n",
    "        X_val=X_train[val_index]\n",
    "        y_train=Y_train[train_index]\n",
    "        y_val=Y_train[val_index]\n",
    "        scaler = StandardScaler()\n",
    "        XS_train=scaler.fit_transform(X_train1) \n",
    "        XS_val=scaler.transform(X_val)\n",
    "        poly = PolynomialFeatures(degree)\n",
    "        XSK_train=poly.fit_transform(XS_train)\n",
    "        XSK_test=poly.transform(XS_val)\n",
    "\n",
    "        clf = LogisticRegression(random_state=0, solver='lbfgs').fit(XSK_train,y_train)\n",
    "        y_pred=clf.predict(XSK_test) \n",
    "        #print(gamma,accuracy_score(y_val, y_pred))\n",
    "        c+=accuracy_score(y_val, y_pred)\n",
    "        #c+=f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    accuracyavg=c/n_splits\n",
    "    return accuracyavg\n",
    "\n",
    "def classifier_polynomial_features(degree,X_train,X_test,y_train,y_test):\n",
    "    scaler = StandardScaler()\n",
    "    XS_train=scaler.fit_transform(X_train) \n",
    "    XS_test=scaler.transform(X_test)\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    XSK_train=poly.fit_transform(XS_train)\n",
    "    XSK_test=poly.transform(XS_test)\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs').fit(XSK_train,y_train)\n",
    "    y_pred=clf.predict(XSK_test) \n",
    "    string=\"Poly_Features\"\n",
    "    \n",
    "    return accuracy_score(y_test, y_pred),string\n",
    "    #return f1_score(y_test, y_pred,average=\"macro\"),string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sayyapureddi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of rbf_kernel = 0.883333333333\n",
      "Validation Accuracy of polynomial_kernel = 0.95\n",
      "Validation Accuracy of polynomial_features = 0.916666666667\n",
      "\n",
      "Accuracy of rbf_kernel=0.833333333333,gamma=0.8 \n",
      "Accuracy of polynomial_kernel =0.966666666667,degree=2, gamma=2.3 \n",
      "Accuracy of polynomial_features =0.933333333333,degree=2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.2)\n",
    "\n",
    "gamma = [2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]\n",
    "gamma_rbf = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9,3.0,4,5,6,7,8,9,10]\n",
    "degree= [2,3,4,5,6,7,8,9,10,15,20,25]\n",
    "#degree= [2,3]\n",
    "#print(type(gamma))\n",
    "#gamma_val=kernel_rbf(gamma_rbf,X_train,y_train)\n",
    "\n",
    "log_dict={}\n",
    "val_log_dict={}\n",
    "\n",
    "n_splits=10\n",
    "skf = KFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "#Hyperparameter tuning.\n",
    "hyper_param_tup_rbf=transform_rbf(gamma_rbf,X_train,y_train,skf,n_splits)\n",
    "hyper_param_tup_polyk=transform_poly(degree,gamma_rbf,X_train, y_train,skf,n_splits)\n",
    "hyper_param_tup_poly_feat=transform_poly_features(degree, X_train, y_train,skf,n_splits)\n",
    "\n",
    "\n",
    "skf1 = KFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "#Validation using K fold CV.\n",
    "rbf_val_acc=val_rbf(X_train, y_train,skf1,hyper_param_tup_rbf,n_splits)\n",
    "print(\"Validation Accuracy of rbf_kernel =\", rbf_val_acc)\n",
    "polyk_val_acc=val_poly( X_train, y_train,skf1, hyper_param_tup_polyk, n_splits)\n",
    "print(\"Validation Accuracy of polynomial_kernel =\", polyk_val_acc)\n",
    "poly_feat_val_acc=val_poly_features(X_train, y_train,skf1, hyper_param_tup_poly_feat, n_splits)\n",
    "print(\"Validation Accuracy of polynomial_features =\", poly_feat_val_acc)\n",
    "\n",
    "print() # for formatting\n",
    "## Running the algo on the test data\n",
    "rbf_acc,string = classifier_rbf(hyper_param_tup_rbf,X_train,X_test,y_train,y_test)\n",
    "print(\"Accuracy of rbf_kernel=%s,gamma=%s \" %(rbf_acc,hyper_param_tup_rbf))\n",
    "key1=(string,hyper_param_tup_rbf)\n",
    "log_dict[key1]=rbf_acc\n",
    "val_log_dict[key1]=rbf_val_acc\n",
    "\n",
    "polyk_acc,string=classifier_polynomial_kernel(hyper_param_tup_polyk,X_train,X_test,y_train,y_test)\n",
    "print(\"Accuracy of polynomial_kernel =%s,degree=%s, gamma=%s \" %(polyk_acc,hyper_param_tup_polyk[0], hyper_param_tup_polyk[1]))\n",
    "key2=(string,hyper_param_tup_polyk)\n",
    "log_dict[key2]=polyk_acc\n",
    "val_log_dict[key2]=polyk_val_acc\n",
    "\n",
    "poly_feat_acc,string=classifier_polynomial_features(hyper_param_tup_poly_feat,X_train,X_test,y_train,y_test)\n",
    "print(\"Accuracy of polynomial_features =%s,degree=%s\" %(poly_feat_acc,hyper_param_tup_poly_feat))\n",
    "key3=(string,hyper_param_tup_poly_feat)\n",
    "log_dict[key3]=poly_feat_acc\n",
    "val_log_dict[key3]=poly_feat_val_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('rbf_kernel', 0.8): 0.83333333333333337, ('Poly_kernel', (2, 2.3)): 0.96666666666666667, ('Poly_Features', 2): 0.93333333333333335}\n"
     ]
    }
   ],
   "source": [
    "print(log_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The polynomial_kernel seems to have the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBCQhHD-m1KS"
   },
   "source": [
    "## Question 2 (10 pts)\n",
    "Write the explicit constraints (without using any vectorial notation, as a summation of single variables multiplied by a constant + bias term) of the Support Vector Machine to classify correctly iris dataset (Iris-Versicolor vs. others). In particular use 5 points in Iris-Versicolor, 2 points for iris-setosa, and 3 points for iris Virginia.\n",
    "Please show the points you selected and after the constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"iris.csv\",header=None)\n",
    "data.columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_versicolor =data[data[\"class\"]=='Iris-versicolor'].sample(5)\n",
    "data_setosa =data[data[\"class\"]=='Iris-setosa'].sample(2)\n",
    "data_virginica =data[data[\"class\"]=='Iris-virginica'].sample(3)\n",
    "data_sample=pd.concat([data_versicolor,data_setosa,data_virginica])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected points are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width            class\n",
       "66            5.6          3.0           4.5          1.5  Iris-versicolor\n",
       "62            6.0          2.2           4.0          1.0  Iris-versicolor\n",
       "70            5.9          3.2           4.8          1.8  Iris-versicolor\n",
       "69            5.6          2.5           3.9          1.1  Iris-versicolor\n",
       "82            5.8          2.7           3.9          1.2  Iris-versicolor\n",
       "11            4.8          3.4           1.6          0.2      Iris-setosa\n",
       "6             4.6          3.4           1.4          0.3      Iris-setosa\n",
       "115           6.4          3.2           5.3          2.3   Iris-virginica\n",
       "106           4.9          2.5           4.5          1.7   Iris-virginica\n",
       "118           7.7          2.6           6.9          2.3   Iris-virginica"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below equations show how the weights and the bias are calculated from the selected points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versicolor =1\n",
    "\n",
    "$$\\ w1*5.6 + w2*3.0 + w3*4.5 + w4*1.5 + b \\geq  1 $$\n",
    "$$\\ w1*6.0 + w2*2.2 + w3*4.0 + w4*1.0 + b \\geq  1 $$\n",
    "$$\\ w1*5.9 + w2*3.2 + w3*4.8 + w4*1.8 + b \\geq  1 $$\n",
    "$$\\ w1*5.6 + w2*2.5 + w3*3.9 + w4*1.1 + b \\geq  1 $$\n",
    "$$\\ w1*5.8 + w2*2.7 + w3*3.9 + w4*1.1 + b \\geq  1 $$\n",
    "\n",
    "### Others =-1\n",
    "$$\\ w1*4.8 + w2*3.4 + w3*1.6 + w4*0.2 + b \\leq -1 $$\n",
    "$$\\ w1*4.6 + w2*3.4 + w3*1.4 + w4*0.3 + b \\leq -1 $$\n",
    "$$\\ w1*6.4 + w2*3.2 + w3*5.3 + w4*2.3 + b \\leq -1 $$\n",
    "$$\\ w1*4.9 + w2*2.5 + w3*4.5 + w4*1.7 + b \\leq -1 $$\n",
    "$$\\ w1*7.7 + w2*2.6 + w3*6.9 + w4*2.3 + b \\leq -1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mHkf54Sm1KT"
   },
   "source": [
    "# Extra Points ( 5pts)\n",
    "An unbalanced dataset (e.g. 95% vs 5%) can be problematic even in the training phase. The learned function can be trivial, e.g. always predicting one class.\n",
    "A possible solution can have a weight for each point in the way that making a mistake in the minority class will coun more w.r.t. the other. Please redefine the likelihood of the logistic regression to consider these weights for each point. Please compute the log-likelihood and its derivatives.\n",
    "In addition, add to the negative log-likelihood the norm of W (sum of the square of each component) and compute the derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqC9w6tkm1KU"
   },
   "source": [
    "### The Anwser to this question is attached as a seperate scanned file in the submission folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PS3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
